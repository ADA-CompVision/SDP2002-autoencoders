{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee1474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the root directory of the dataset\n",
    "root_dir = \"data/\"\n",
    "\n",
    "# Define the batch size for the data loader\n",
    "batch_size = 32\n",
    "\n",
    "# Define the transforms for the dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    \n",
    "   \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_dataset = ImageFolder(root_dir + \"train\", transform=train_transforms)\n",
    "val_dataset = ImageFolder(root_dir + \"validation\", transform=val_transforms)\n",
    "\n",
    "# Create the train and validation data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05eb8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoding_dim = encoding_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        encoded = F.relu(self.conv4(x))\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, x):\n",
    "        # Decoder\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        decoded = torch.sigmoid(self.deconv4(x))\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a352284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder=Autoencoder(256*256*256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d981f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5a0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(autoencoder.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad4d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Training Loss: 0.088, Validation Loss: 0.085\n",
      "Epoch [2], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [3], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [4], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [5], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [6], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [7], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [8], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [9], Training Loss: 0.085, Validation Loss: 0.085\n",
      "Epoch [10], Training Loss: 0.085, Validation Loss: 0.085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the ImageClassifier\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    running_loss = 0.0\n",
    "    autoencoder.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss/len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    running_loss = 0.0\n",
    "    autoencoder.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, _ = data\n",
    "            outputs, _ = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            running_loss += loss.item()\n",
    "        val_loss = running_loss/len(val_loader)\n",
    "       \n",
    "    print('Epoch [%d], Training Loss: %.3f, Validation Loss: %.3f' % (epoch+1, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0824809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is named 'model'\n",
    "torch.save(autoencoder.state_dict(), 'auto.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a3c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Define the fully-connected layers\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # Flatten the output of the last convolutional layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass the flattened output through the fully-connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b578a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a classifier and load the weights of the encoder from the autoencoder\n",
    "classifier = Classifier(16*16*256)\n",
    "classifier.conv1.load_state_dict(autoencoder.conv1.state_dict())\n",
    "classifier.conv2.load_state_dict(autoencoder.conv2.state_dict())\n",
    "classifier.conv3.load_state_dict(autoencoder.conv3.state_dict())\n",
    "classifier.conv4.load_state_dict(autoencoder.conv4.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81aa25d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 22.1162, Training Accuracy: 17.9973, Validation Loss: 0.6320, Validation Accuracy: 0.6407\n",
      "Epoch [2/10], Training Loss: 19.2683, Training Accuracy: 21.5394, Validation Loss: 0.5681, Validation Accuracy: 0.7014\n",
      "Epoch [3/10], Training Loss: 16.4586, Training Accuracy: 23.7423, Validation Loss: 0.4651, Validation Accuracy: 0.7724\n",
      "Epoch [4/10], Training Loss: 12.7222, Training Accuracy: 26.1188, Validation Loss: 0.4033, Validation Accuracy: 0.8165\n",
      "Epoch [5/10], Training Loss: 8.5697, Training Accuracy: 28.3378, Validation Loss: 0.4214, Validation Accuracy: 0.8199\n",
      "Epoch [6/10], Training Loss: 5.3865, Training Accuracy: 29.8331, Validation Loss: 0.4355, Validation Accuracy: 0.8356\n",
      "Epoch [7/10], Training Loss: 3.2609, Training Accuracy: 30.7557, Validation Loss: 0.5288, Validation Accuracy: 0.8426\n",
      "Epoch [8/10], Training Loss: 2.4511, Training Accuracy: 31.1308, Validation Loss: 0.5414, Validation Accuracy: 0.8391\n",
      "Epoch [9/10], Training Loss: 1.9902, Training Accuracy: 31.3298, Validation Loss: 0.6961, Validation Accuracy: 0.8341\n",
      "Epoch [10/10], Training Loss: 1.5155, Training Accuracy: 31.5087, Validation Loss: 0.7862, Validation Accuracy: 0.8456\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Training\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    classifier.train() # set the model to training mode\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_acc += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    classifier.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_acc += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "\n",
    "    # Print the loss and accuracy after every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed78e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), 'classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2420c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "dataset = ImageFolder(root='data2', transform = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    ")\n",
    "\n",
    "# Define the data loader\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce779cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.55%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        # Move the data to the device\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classifier(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b01dafc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: dog\n"
     ]
    }
   ],
   "source": [
    "# Load the image and preprocess it\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = Image.open('train/cat.90.jpg')\n",
    "image = image.resize((256, 256)) # Resize to the same size used during training\n",
    "image = np.array(image)/255.0 # Normalize the pixel values between 0 and 1\n",
    "image = np.transpose(image, (2, 0, 1)) # Convert to the shape (C, H, W) expected by PyTorch\n",
    "image_tensor = torch.tensor(image).float() # Convert to tensor and change the data type to float\n",
    "\n",
    "# Make a prediction\n",
    "output = classifier(image_tensor.unsqueeze(0)) # Add an extra dimension to represent the batch size\n",
    "probabilities = torch.softmax(output, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "# Print the predicted class and the associated probability\n",
    "print(f\"Predicted class: {'dog' if predicted_class == 0 else 'cat'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4c1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
